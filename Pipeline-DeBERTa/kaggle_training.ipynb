{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline-DeBERTa Training for DimABSA 2026\n",
    "\n",
    "**Subtask 2**: Dimensional Aspect Sentiment Triplet Extraction (DimASTE)\n",
    "\n",
    "**Model**: Pipeline-based approach with DeBERTa-v3-base encoder\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "- GPU: T4 or P100 recommended\n",
    "- Training time: ~2-3 hours for both domains\n",
    "- Output: Trained models + predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch sentencepiece protobuf spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone/Upload Code\n",
    "\n",
    "**Option A**: Upload Pipeline-DeBERTa folder as Kaggle dataset\n",
    "\n",
    "**Option B**: Clone from GitHub (if you pushed it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If uploaded as Kaggle dataset, copy to working directory\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Adjust path to your Kaggle dataset\n",
    "# !cp -r /kaggle/input/pipeline-deberta/* /kaggle/working/\n",
    "\n",
    "# Or if cloning from GitHub:\n",
    "# !git clone https://github.com/YOUR-USERNAME/dimabsa-pipeline-deberta.git\n",
    "# %cd dimabsa-pipeline-deberta\n",
    "\n",
    "# For now, assume code is in /kaggle/working/Pipeline-DeBERTa\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DimABSA 2026 dataset\n",
    "!mkdir -p data/track_a/subtask_2/eng\n",
    "\n",
    "# Restaurant data\n",
    "!wget -q https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset/track_a/subtask_2/eng/eng_restaurant_train_alltasks.jsonl \\\n",
    "    -O data/track_a/subtask_2/eng/eng_restaurant_train_alltasks.jsonl\n",
    "!wget -q https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset/track_a/subtask_2/eng/eng_restaurant_dev_task2.jsonl \\\n",
    "    -O data/track_a/subtask_2/eng/eng_restaurant_dev_task2.jsonl\n",
    "\n",
    "# Laptop data\n",
    "!wget -q https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks.jsonl \\\n",
    "    -O data/track_a/subtask_2/eng/eng_laptop_train_alltasks.jsonl\n",
    "!wget -q https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset/track_a/subtask_2/eng/eng_laptop_dev_task2.jsonl \\\n",
    "    -O data/track_a/subtask_2/eng/eng_laptop_dev_task2.jsonl\n",
    "\n",
    "print(\"âœ“ Dataset downloaded\")\n",
    "!ls -lh data/track_a/subtask_2/eng/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Restaurant Domain\n",
    "\n",
    "**Dataset**: 2,284 training samples, 200 dev samples\n",
    "\n",
    "**Time**: ~30-45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_task2\\&3_trainer_multilingual.py \\\n",
    "  --task 2 \\\n",
    "  --domain res \\\n",
    "  --language eng \\\n",
    "  --train_data data/track_a/subtask_2/eng/eng_restaurant_train_alltasks.jsonl \\\n",
    "  --infer_data data/track_a/subtask_2/eng/eng_restaurant_dev_task2.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8 \\\n",
    "  --learning_rate 1e-3 \\\n",
    "  --tuning_bert_rate 1e-5 \\\n",
    "  --inference_beta 0.9 \\\n",
    "  --gpu True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Restaurant Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check predictions\n",
    "pred_file = \"tasks/subtask_2/pred_eng_restaurant.jsonl\"\n",
    "with open(pred_file, 'r') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"\\nFirst 3 predictions:\")\n",
    "for i, pred in enumerate(predictions[:3]):\n",
    "    print(f\"\\n{i+1}. ID: {pred['ID']}\")\n",
    "    print(f\"   Triplets: {len(pred['Triplet'])}\")\n",
    "    if pred['Triplet']:\n",
    "        print(f\"   Example: {pred['Triplet'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Laptop Domain\n",
    "\n",
    "**Dataset**: 4,076 training samples, 200 dev samples\n",
    "\n",
    "**Time**: ~60-90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_task2\\&3_trainer_multilingual.py \\\n",
    "  --task 2 \\\n",
    "  --domain lap \\\n",
    "  --language eng \\\n",
    "  --train_data data/track_a/subtask_2/eng/eng_laptop_train_alltasks.jsonl \\\n",
    "  --infer_data data/track_a/subtask_2/eng/eng_laptop_dev_task2.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8 \\\n",
    "  --learning_rate 1e-3 \\\n",
    "  --tuning_bert_rate 1e-5 \\\n",
    "  --inference_beta 0.9 \\\n",
    "  --gpu True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check Laptop Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions\n",
    "pred_file = \"tasks/subtask_2/pred_eng_laptop.jsonl\"\n",
    "with open(pred_file, 'r') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"\\nFirst 3 predictions:\")\n",
    "for i, pred in enumerate(predictions[:3]):\n",
    "    print(f\"\\n{i+1}. ID: {pred['ID']}\")\n",
    "    print(f\"   Triplets: {len(pred['Triplet'])}\")\n",
    "    if pred['Triplet']:\n",
    "        print(f\"   Example: {pred['Triplet'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(pred_file):\n",
    "    \"\"\"Validate prediction format for Subtask 2\"\"\"\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "    \n",
    "    errors = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        # Check required keys\n",
    "        if 'ID' not in pred:\n",
    "            errors.append(f\"Line {i}: Missing 'ID'\")\n",
    "        if 'Triplet' not in pred:\n",
    "            errors.append(f\"Line {i}: Missing 'Triplet'\")\n",
    "            continue\n",
    "        \n",
    "        # Check triplet format\n",
    "        for j, triplet in enumerate(pred['Triplet']):\n",
    "            if 'Aspect' not in triplet:\n",
    "                errors.append(f\"Line {i}, Triplet {j}: Missing 'Aspect'\")\n",
    "            if 'Opinion' not in triplet:\n",
    "                errors.append(f\"Line {i}, Triplet {j}: Missing 'Opinion'\")\n",
    "            if 'VA' not in triplet:\n",
    "                errors.append(f\"Line {i}, Triplet {j}: Missing 'VA'\")\n",
    "            else:\n",
    "                # Validate VA format\n",
    "                va = triplet['VA']\n",
    "                if '#' not in va:\n",
    "                    errors.append(f\"Line {i}, Triplet {j}: VA missing '#' separator\")\n",
    "                else:\n",
    "                    parts = va.split('#')\n",
    "                    if len(parts) != 2:\n",
    "                        errors.append(f\"Line {i}, Triplet {j}: VA should have exactly 2 values\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            v, a = float(parts[0]), float(parts[1])\n",
    "                            if not (1.0 <= v <= 9.0):\n",
    "                                errors.append(f\"Line {i}, Triplet {j}: Valence {v} out of range [1,9]\")\n",
    "                            if not (1.0 <= a <= 9.0):\n",
    "                                errors.append(f\"Line {i}, Triplet {j}: Arousal {a} out of range [1,9]\")\n",
    "                        except ValueError:\n",
    "                            errors.append(f\"Line {i}, Triplet {j}: VA values not numeric\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"âŒ Found {len(errors)} errors:\")\n",
    "        for err in errors[:10]:  # Show first 10\n",
    "            print(f\"  - {err}\")\n",
    "        if len(errors) > 10:\n",
    "            print(f\"  ... and {len(errors)-10} more\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"âœ… All {len(predictions)} predictions valid!\")\n",
    "        return True\n",
    "\n",
    "print(\"Validating Restaurant predictions:\")\n",
    "validate_predictions(\"tasks/subtask_2/pred_eng_restaurant.jsonl\")\n",
    "\n",
    "print(\"\\nValidating Laptop predictions:\")\n",
    "validate_predictions(\"tasks/subtask_2/pred_eng_laptop.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file with all results\n",
    "!mkdir -p results\n",
    "!cp model/*.pth results/\n",
    "!cp tasks/subtask_2/*.jsonl results/\n",
    "!cp log/*.log results/\n",
    "\n",
    "!zip -r pipeline_deberta_results.zip results/\n",
    "\n",
    "print(\"âœ“ Results packaged\")\n",
    "print(\"\\nDownload: pipeline_deberta_results.zip\")\n",
    "print(\"\\nContents:\")\n",
    "!ls -lh results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Restaurant\n",
    "with open(\"tasks/subtask_2/pred_eng_restaurant.jsonl\", 'r') as f:\n",
    "    res_preds = [json.loads(line) for line in f]\n",
    "    res_triplets = sum(len(p['Triplet']) for p in res_preds)\n",
    "\n",
    "print(f\"\\nðŸ“Š Restaurant Domain:\")\n",
    "print(f\"  - Predictions: {len(res_preds)}\")\n",
    "print(f\"  - Total triplets: {res_triplets}\")\n",
    "print(f\"  - Avg triplets/sample: {res_triplets/len(res_preds):.2f}\")\n",
    "print(f\"  - Model: model/task2_res_eng.pth\")\n",
    "\n",
    "# Laptop\n",
    "with open(\"tasks/subtask_2/pred_eng_laptop.jsonl\", 'r') as f:\n",
    "    lap_preds = [json.loads(line) for line in f]\n",
    "    lap_triplets = sum(len(p['Triplet']) for p in lap_preds)\n",
    "\n",
    "print(f\"\\nðŸ’» Laptop Domain:\")\n",
    "print(f\"  - Predictions: {len(lap_preds)}\")\n",
    "print(f\"  - Total triplets: {lap_triplets}\")\n",
    "print(f\"  - Avg triplets/sample: {lap_triplets/len(lap_preds):.2f}\")\n",
    "print(f\"  - Model: model/task2_lap_eng.pth\")\n",
    "\n",
    "print(f\"\\nâœ… Training Complete!\")\n",
    "print(f\"\\nðŸ“¦ Download: pipeline_deberta_results.zip\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
