{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESS Training for DimABSA 2026\n",
    "\n",
    "**Subtask 2**: Dimensional Aspect Sentiment Triplet Extraction\n",
    "\n",
    "**Model**: DESS (Dual-channel Enhanced Sentiment Span) with DeBERTa-v3-base\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Requirements\n",
    "- **GPU**: T4 or P100 (enable in Settings ‚Üí Accelerator)\n",
    "- **Time**: ~4-5 hours for full training\n",
    "- **Internet**: Required for downloading code\n",
    "- **Memory**: ~10.7 GB (safe for 16GB GPU with batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/VishalRepos/dimabsa-2026.git\n",
    "%cd dimabsa-2026/DESS/Codebase\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers torch sentencepiece protobuf spacy torch-geometric tensorboardX\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: GPU not available! Enable GPU in Settings ‚Üí Accelerator ‚Üí GPU T4\")\n",
    "\n",
    "# Check data\n",
    "print(\"\\nChecking data...\")\n",
    "data_path = \"./data/dimabsa_combined/train_dep_triple_polarity_result.json\"\n",
    "if os.path.exists(data_path):\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Training data found: {len(data)} samples\")\n",
    "    print(f\"  Max entities: {max(len(d['entities']) for d in data)}\")\n",
    "    print(f\"  Avg entities: {sum(len(d['entities']) for d in data) / len(data):.2f}\")\n",
    "else:\n",
    "    print(f\"‚ùå Data not found at {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train DESS Model\n",
    "\n",
    "**Configuration**:\n",
    "- Dataset: dimabsa_combined (3,727 samples)\n",
    "- Model: DeBERTa-v3-base\n",
    "- Batch size: 1 (memory safe)\n",
    "- Epochs: 10\n",
    "- Memory: ~10.7 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"‚úì GPU memory cleared\")\n",
    "\n",
    "# Train DESS\n",
    "!python train.py \\\n",
    "  --dataset dimabsa_combined \\\n",
    "  --pretrained_deberta_name microsoft/deberta-v3-base \\\n",
    "  --train_batch_size 1 \\\n",
    "  --eval_batch_size 1 \\\n",
    "  --epochs 10 \\\n",
    "  --lr 5e-5 \\\n",
    "  --max_span_size 10 \\\n",
    "  --neg_entity_count 100 \\\n",
    "  --neg_triple_count 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check saved models\n",
    "print(\"Saved models:\")\n",
    "if os.path.exists(\"./savemodels\"):\n",
    "    models = [f for f in os.listdir(\"./savemodels\") if f.endswith(\".pth\")]\n",
    "    for model in models:\n",
    "        size = os.path.getsize(f\"./savemodels/{model}\") / 1e6\n",
    "        print(f\"  {model}: {size:.1f} MB\")\n",
    "else:\n",
    "    print(\"  No models found\")\n",
    "\n",
    "# Check logs\n",
    "print(\"\\nLog files:\")\n",
    "if os.path.exists(\"./log\"):\n",
    "    for root, dirs, files in os.walk(\"./log\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\") or file.endswith(\".json\"):\n",
    "                print(f\"  {os.path.join(root, file)}\")\n",
    "else:\n",
    "    print(\"  No logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "!python predict.py \\\n",
    "  --dataset dimabsa_combined \\\n",
    "  --pretrained_deberta_name microsoft/deberta-v3-base \\\n",
    "  --model_path ./savemodels/best_model.pth \\\n",
    "  --eval_batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Package Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "!mkdir -p results\n",
    "\n",
    "# Copy outputs\n",
    "!cp -r savemodels/*.pth results/ 2>/dev/null || echo \"No models to copy\"\n",
    "!cp -r log/* results/ 2>/dev/null || echo \"No logs to copy\"\n",
    "\n",
    "# Create zip\n",
    "!zip -r dess_results.zip results/\n",
    "\n",
    "print(\"\\n‚úÖ Results packaged!\")\n",
    "print(\"\\nüì¶ Download: dess_results.zip\")\n",
    "print(\"\\nContents:\")\n",
    "!ls -lh results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DESS TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dataset info\n",
    "data_path = \"./data/dimabsa_combined/train_dep_triple_polarity_result.json\"\n",
    "if os.path.exists(data_path):\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"\\nüìä Dataset:\")\n",
    "    print(f\"  Total samples: {len(data)}\")\n",
    "    print(f\"  Max entities: {max(len(d['entities']) for d in data)}\")\n",
    "    print(f\"  Avg entities: {sum(len(d['entities']) for d in data) / len(data):.2f}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\nü§ñ Model:\")\n",
    "print(f\"  Architecture: DESS (Dual-channel Enhanced Sentiment Span)\")\n",
    "print(f\"  Backbone: DeBERTa-v3-base\")\n",
    "print(f\"  Batch size: 1\")\n",
    "print(f\"  Epochs: 10\")\n",
    "\n",
    "# Check if training completed\n",
    "if os.path.exists(\"./savemodels\"):\n",
    "    models = [f for f in os.listdir(\"./savemodels\") if f.endswith(\".pth\")]\n",
    "    if models:\n",
    "        print(f\"\\n‚úÖ Training Complete!\")\n",
    "        print(f\"  Models saved: {len(models)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No models saved\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Training may not have completed\")\n",
    "\n",
    "print(\"\\nüì• Download: dess_results.zip (from Output tab)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Memory Usage\n",
    "- DESS uses **1 forward pass** (vs 6 in Pipeline)\n",
    "- Memory: ~10.7 GB with batch_size=1\n",
    "- Safe for T4 (15GB) and P100 (16GB)\n",
    "\n",
    "### Advantages over Pipeline-DeBERTa\n",
    "- ‚úì Uses full dataset (no filtering needed)\n",
    "- ‚úì More efficient (1 pass vs 6)\n",
    "- ‚úì Dual-channel architecture (syntax + semantics)\n",
    "- ‚úì Span-based extraction\n",
    "\n",
    "### Training Time\n",
    "- ~4-5 hours for 10 epochs\n",
    "- 3,727 samples with batch_size=1\n",
    "\n",
    "### Troubleshooting\n",
    "- If OOM: Already using batch_size=1 (minimum)\n",
    "- If slow: Expected with batch_size=1\n",
    "- If errors: Check GPU is enabled in Settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
