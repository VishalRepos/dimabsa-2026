{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DimABSA Training - DESS Model with VA Regression\n",
    "\n",
    "**Task**: Subtask 2 - Dimensional Aspect Sentiment Triplet Extraction (DimASTE)\n",
    "\n",
    "**Model**: DESS (Dual-channel Enhanced Sentiment Span) adapted for VA regression\n",
    "\n",
    "**Dataset**: Combined Restaurant + Laptop (3,727 training samples)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Upload Required Files to Kaggle Dataset\n",
    "\n",
    "Create a Kaggle dataset with these files:\n",
    "```\n",
    "dimabsa-dess-data/\n",
    "â”œâ”€â”€ DESS/Codebase/\n",
    "â”‚   â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ trainer/\n",
    "â”‚   â”œâ”€â”€ data/\n",
    "â”‚   â”‚   â”œâ”€â”€ dimabsa_combined/\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ train_dep_triple_polarity_result.json\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ test_dep_triple_polarity_result.json\n",
    "â”‚   â”‚   â””â”€â”€ types_va.json\n",
    "â”‚   â”œâ”€â”€ Parameter.py\n",
    "â”‚   â””â”€â”€ train.py\n",
    "```\n",
    "\n",
    "### 2. Enable GPU\n",
    "- Go to Settings â†’ Accelerator â†’ GPU T4 x2 (or P100)\n",
    "\n",
    "### 3. Run All Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers scikit-learn tqdm\n",
    "!pip install -q torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths (adjust based on your Kaggle dataset)\n",
    "DATA_PATH = \"/kaggle/input/dimabsa-dess-data/DESS/Codebase\"\n",
    "sys.path.insert(0, DATA_PATH)\n",
    "\n",
    "# Verify data\n",
    "train_path = f\"{DATA_PATH}/data/dimabsa_combined/train_dep_triple_polarity_result.json\"\n",
    "test_path = f\"{DATA_PATH}/data/dimabsa_combined/test_dep_triple_polarity_result.json\"\n",
    "\n",
    "train_data = json.load(open(train_path))\n",
    "test_data = json.load(open(test_path))\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nSample structure:\")\n",
    "print(f\"  Tokens: {len(train_data[0]['tokens'])}\")\n",
    "print(f\"  Entities: {len(train_data[0]['entities'])}\")\n",
    "print(f\"  Sentiments: {len(train_data[0]['sentiments'])}\")\n",
    "if train_data[0]['sentiments']:\n",
    "    print(f\"  Sample VA: {train_data[0]['sentiments'][0]['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    'dataset': 'dimabsa_combined',\n",
    "    'model_type': 'deberta-v3-base',  # Use base model for Kaggle (faster)\n",
    "    'batch_size': 4,  # Adjust based on GPU memory\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 5e-5,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_span_size': 10,\n",
    "    'neg_entity_count': 100,\n",
    "    'neg_senti_count': 100,\n",
    "    'save_path': '/kaggle/working/checkpoints',\n",
    "    'log_path': '/kaggle/working/logs',\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['save_path'], exist_ok=True)\n",
    "os.makedirs(CONFIG['log_path'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from models.D2E2S_Model import D2E2SModel\n",
    "from trainer.input_reader import JsonInputReader\n",
    "from trainer.loss import D2E2SLoss\n",
    "from Parameter import train_argparser\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load data reader\n",
    "types_path = f\"{DATA_PATH}/data/types_va.json\"\n",
    "input_reader = JsonInputReader(\n",
    "    types_path=types_path,\n",
    "    tokenizer=tokenizer,\n",
    "    neg_entity_count=CONFIG['neg_entity_count'],\n",
    "    neg_senti_count=CONFIG['neg_senti_count'],\n",
    "    max_span_size=CONFIG['max_span_size']\n",
    ")\n",
    "\n",
    "# Read datasets\n",
    "dataset_paths = {\n",
    "    'train': train_path,\n",
    "    'test': test_path\n",
    "}\n",
    "input_reader.read(dataset_paths)\n",
    "\n",
    "train_dataset = input_reader.get_dataset('train')\n",
    "test_dataset = input_reader.get_dataset('test')\n",
    "\n",
    "print(f\"\\nDatasets loaded:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "print(f\"  Entity types: {input_reader.entity_type_count}\")\n",
    "print(f\"  Sentiment types: {input_reader.sentiment_type_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Mock args for model initialization\n",
    "class Args:\n",
    "    size_embedding = 25\n",
    "    prop_drop = 0.1\n",
    "    freeze_transformer = False\n",
    "    drop_out_rate = 0.5\n",
    "    is_bidirect = True\n",
    "    lstm_layers = 1\n",
    "    hidden_dim = 768\n",
    "    mem_dim = 300\n",
    "    emb_dim = 1536\n",
    "    batch_size = CONFIG['batch_size']\n",
    "    deberta_feature_dim = 768\n",
    "    gcn_dim = 300\n",
    "    gcn_dropout = 0.5\n",
    "    span_generator = \"Max\"\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = D2E2SModel(\n",
    "    config=config,\n",
    "    cls_token=tokenizer.cls_token_id,\n",
    "    sentiment_types=2,  # VA regression: 2 outputs\n",
    "    entity_types=input_reader.entity_type_count,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataset) // CONFIG['batch_size'] * CONFIG['epochs']\n",
    "num_warmup_steps = int(num_training_steps * CONFIG['warmup_proportion'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "entity_criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "senti_criterion = torch.nn.MSELoss(reduction='none')  # MSE for VA regression\n",
    "\n",
    "loss_fn = D2E2SLoss(\n",
    "    senti_criterion=senti_criterion,\n",
    "    entity_criterion=entity_criterion,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    max_grad_norm=CONFIG['max_grad_norm']\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']})\")\n",
    "print(f\"Scheduler: Linear warmup ({num_warmup_steps} steps)\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Entity loss: CrossEntropyLoss\")\n",
    "print(f\"Sentiment loss: MSELoss (VA regression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, loss_fn, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    for batch in pbar:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        entity_logits, senti_logits, batch_loss = model(\n",
    "            encodings=batch['encodings'],\n",
    "            context_masks=batch['context_masks'],\n",
    "            entity_masks=batch['entity_masks'],\n",
    "            entity_sizes=batch['entity_sizes'],\n",
    "            sentiments=batch['sentiments'],\n",
    "            senti_masks=batch['senti_masks'],\n",
    "            adj=batch['adj']\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn.compute(\n",
    "            entity_logits, senti_logits, batch_loss,\n",
    "            batch['entity_types'], batch['senti_types'],\n",
    "            batch['entity_sample_masks'], batch['senti_sample_masks']\n",
    "        )\n",
    "        \n",
    "        total_loss += loss\n",
    "        pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Batches per epoch: {len(train_dataloader)}\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    avg_loss = train_epoch(model, train_dataloader, loss_fn, epoch)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Log\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}:\")\n",
    "    print(f\"  Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': avg_loss,\n",
    "        'time': epoch_time\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        checkpoint_path = f\"{CONFIG['save_path']}/best_model.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  âœ… Best model saved (loss: {best_loss:.4f})\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training completed!\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = f\"{CONFIG['log_path']}/training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# Plot training curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [h['epoch'] for h in training_history]\n",
    "losses = [h['loss'] for h in training_history]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{CONFIG['log_path']}/training_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curve saved to: {CONFIG['log_path']}/training_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f\"{CONFIG['save_path']}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Best model loaded (epoch {checkpoint['epoch']}, loss {checkpoint['loss']:.4f})\")\n",
    "\n",
    "# Quick evaluation on test set\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=test_dataset.collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning evaluation on {len(test_dataset)} test samples...\")\n",
    "\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        entity_clf, senti_clf, sentiments = model(\n",
    "            encodings=batch['encodings'],\n",
    "            context_masks=batch['context_masks'],\n",
    "            entity_masks=batch['entity_masks'],\n",
    "            entity_sizes=batch['entity_sizes'],\n",
    "            entity_spans=batch['entity_spans'],\n",
    "            entity_sample_masks=batch['entity_sample_masks'],\n",
    "            adj=batch['adj'],\n",
    "            evaluate=True\n",
    "        )\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions.append({\n",
    "            'entity_clf': entity_clf.cpu(),\n",
    "            'senti_clf': senti_clf.cpu(),\n",
    "            'sentiments': sentiments.cpu()\n",
    "        })\n",
    "\n",
    "print(f\"âœ… Evaluation complete! {len(all_predictions)} batches processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trained model is saved at:\n",
    "print(f\"Trained model location: {CONFIG['save_path']}/best_model.pt\")\n",
    "print(f\"Training logs: {CONFIG['log_path']}/\")\n",
    "print(\"\\nDownload these files from Kaggle output to use for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
