{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline-DeBERTa: Subtask 2 Submission Generation\n",
    "## DimABSA 2026 - Track A\n",
    "\n",
    "Trains restaurant + laptop models, runs inference on **test** data, produces submission-ready JSONL files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "!rm -rf dimabsa-2026\n",
    "!git clone https://github.com/VishalRepos/dimabsa-2026.git\n",
    "%cd dimabsa-2026/Pipeline-DeBERTa\n",
    "\n",
    "# Verify test data exists\n",
    "import os\n",
    "test_dir = '../DimABSA2026/task-dataset/track_a/subtask_2/eng/'\n",
    "for f in ['eng_restaurant_test_task2.jsonl', 'eng_laptop_test_task2.jsonl',\n",
    "          'eng_restaurant_train_alltasks_filtered.jsonl', 'eng_laptop_train_alltasks_filtered.jsonl']:\n",
    "    path = os.path.join(test_dir, f)\n",
    "    status = '✓' if os.path.exists(path) else '✗'\n",
    "    print(f'{status} {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.36.0 torch==2.1.0\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Train Restaurant Model + Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on filtered restaurant data, inference on TEST file (not dev)\n",
    "!python 'run_task2&3_trainer_multilingual.py' \\\n",
    "  --task 2 \\\n",
    "  --domain res \\\n",
    "  --language eng \\\n",
    "  --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_train_alltasks_filtered.jsonl \\\n",
    "  --infer_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_test_task2.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Train Laptop Model + Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on filtered laptop data, inference on TEST file\n",
    "!python 'run_task2&3_trainer_multilingual.py' \\\n",
    "  --task 2 \\\n",
    "  --domain lap \\\n",
    "  --language eng \\\n",
    "  --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks_filtered.jsonl \\\n",
    "  --infer_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_test_task2.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Verify Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submission_files = {\n",
    "    'Restaurant': 'tasks/subtask_2/pred_eng_restaurant.jsonl',\n",
    "    'Laptop': 'tasks/subtask_2/pred_eng_laptop.jsonl',\n",
    "}\n",
    "\n",
    "for domain, path in submission_files.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f'✗ {domain}: {path} NOT FOUND')\n",
    "        continue\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    total_triplets = sum(len(d['Triplet']) for d in data)\n",
    "    with_triplets = sum(1 for d in data if d['Triplet'])\n",
    "\n",
    "    # Validate VA format\n",
    "    va_errors = 0\n",
    "    for d in data:\n",
    "        for t in d['Triplet']:\n",
    "            try:\n",
    "                v, a = map(float, t['VA'].split('#'))\n",
    "                if not (1.0 <= v <= 9.0 and 1.0 <= a <= 9.0):\n",
    "                    va_errors += 1\n",
    "            except:\n",
    "                va_errors += 1\n",
    "\n",
    "    print(f'\\n✓ {domain}: {path}')\n",
    "    print(f'  Samples: {len(data)}')\n",
    "    print(f'  Total triplets: {total_triplets}')\n",
    "    print(f'  Samples with triplets: {with_triplets}/{len(data)}')\n",
    "    print(f'  Avg triplets/sample: {total_triplets/len(data):.2f}')\n",
    "    print(f'  VA format errors: {va_errors}')\n",
    "    print(f'  First prediction: {json.dumps(data[0], ensure_ascii=False)[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Cross-check IDs Against Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = {\n",
    "    'Restaurant': '../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_test_task2.jsonl',\n",
    "    'Laptop': '../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_test_task2.jsonl',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for domain, test_path in test_files.items():\n",
    "    pred_path = submission_files[domain]\n",
    "    if not os.path.exists(pred_path) or not os.path.exists(test_path):\n",
    "        print(f'✗ {domain}: files missing')\n",
    "        all_ok = False\n",
    "        continue\n",
    "\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_ids = {json.loads(line)['ID'] for line in f}\n",
    "    with open(pred_path, 'r') as f:\n",
    "        pred_ids = {json.loads(line)['ID'] for line in f}\n",
    "\n",
    "    missing = test_ids - pred_ids\n",
    "    extra = pred_ids - test_ids\n",
    "\n",
    "    if not missing and not extra:\n",
    "        print(f'✓ {domain}: All {len(test_ids)} IDs match')\n",
    "    else:\n",
    "        all_ok = False\n",
    "        if missing:\n",
    "            print(f'✗ {domain}: {len(missing)} missing IDs: {list(missing)[:5]}')\n",
    "        if extra:\n",
    "            print(f'✗ {domain}: {len(extra)} extra IDs: {list(extra)[:5]}')\n",
    "\n",
    "print(f'\\n{\"✓ SUBMISSION READY\" if all_ok else \"✗ FIX ISSUES ABOVE\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Copy Submission Files to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "out_dir = '/kaggle/working/submission_subtask2'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for domain, path in submission_files.items():\n",
    "    if os.path.exists(path):\n",
    "        dest = os.path.join(out_dir, os.path.basename(path))\n",
    "        shutil.copy2(path, dest)\n",
    "        print(f'✓ Copied {path} → {dest}')\n",
    "\n",
    "# Also save models\n",
    "model_dir = '/kaggle/working/trained_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "for f in ['model/task2_res_eng.pth', 'model/task2_lap_eng.pth']:\n",
    "    if os.path.exists(f):\n",
    "        shutil.copy2(f, os.path.join(model_dir, os.path.basename(f)))\n",
    "        print(f'✓ Copied {f}')\n",
    "\n",
    "print(f'\\nDownload from Output panel →')\n",
    "!ls -lh /kaggle/working/submission_subtask2/\n",
    "!ls -lh /kaggle/working/trained_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (Optional) Inference-Only Mode\n",
    "Use these cells if models are already trained and uploaded as a Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and set your model dataset path\n",
    "# MODEL_DATASET = '/kaggle/input/your-model-dataset'\n",
    "# \n",
    "# # Copy pre-trained models into expected location\n",
    "# os.makedirs('model', exist_ok=True)\n",
    "# for f in ['task2_res_eng.pth', 'task2_lap_eng.pth']:\n",
    "#     src = os.path.join(MODEL_DATASET, f)\n",
    "#     if os.path.exists(src):\n",
    "#         shutil.copy2(src, f'model/{f}')\n",
    "#         print(f'✓ Loaded {f}')\n",
    "# \n",
    "# # Run inference only - Restaurant\n",
    "# !python 'run_task2&3_trainer_multilingual.py' \\\n",
    "#   --task 2 --domain res --language eng \\\n",
    "#   --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_train_alltasks_filtered.jsonl \\\n",
    "#   --infer_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_test_task2.jsonl \\\n",
    "#   --bert_model_type microsoft/deberta-v3-base \\\n",
    "#   --mode inference\n",
    "# \n",
    "# # Run inference only - Laptop\n",
    "# !python 'run_task2&3_trainer_multilingual.py' \\\n",
    "#   --task 2 --domain lap --language eng \\\n",
    "#   --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks_filtered.jsonl \\\n",
    "#   --infer_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_test_task2.jsonl \\\n",
    "#   --bert_model_type microsoft/deberta-v3-base \\\n",
    "#   --mode inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
