{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline-DeBERTa: Subtask 3 Submission Generation\n",
    "## DimABSA 2026 - Track A\n",
    "\n",
    "Trains restaurant + laptop models with category classifier (`--task 3`), runs inference on **test** data, produces submission-ready JSONL files with Quadruplets (Aspect, Category, Opinion, VA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "!rm -rf dimabsa-2026\n",
    "!git clone https://github.com/VishalRepos/dimabsa-2026.git\n",
    "%cd dimabsa-2026/Pipeline-DeBERTa\n",
    "\n",
    "# Verify test data and training data exist\n",
    "import os\n",
    "s3_dir = '../DimABSA2026/task-dataset/track_a/subtask_3/eng/'\n",
    "s2_dir = '../DimABSA2026/task-dataset/track_a/subtask_2/eng/'\n",
    "files_to_check = [\n",
    "    (s3_dir, 'eng_restaurant_test_task3.jsonl'),\n",
    "    (s3_dir, 'eng_laptop_test_task3.jsonl'),\n",
    "    # Filtered files are in subtask_2 (identical training data)\n",
    "    (s2_dir, 'eng_restaurant_train_alltasks_filtered.jsonl'),\n",
    "    (s2_dir, 'eng_laptop_train_alltasks_filtered.jsonl'),\n",
    "]\n",
    "for d, f in files_to_check:\n",
    "    path = os.path.join(d, f)\n",
    "    status = '✓' if os.path.exists(path) else '✗'\n",
    "    print(f'{status} {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.36.0 torch==2.1.0\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Train Restaurant Model (Task 3) + Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with --task 3 (adds category classifier)\n",
    "# Uses subtask_2 filtered files (identical content, has Quadruplet+Category)\n",
    "# Infer on subtask_3 test file\n",
    "!python 'run_task2&3_trainer_multilingual.py' \\\n",
    "  --task 3 \\\n",
    "  --domain res \\\n",
    "  --language eng \\\n",
    "  --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_train_alltasks_filtered.jsonl \\\n",
    "  --infer_data ../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_restaurant_test_task3.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Train Laptop Model (Task 3) + Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 'run_task2&3_trainer_multilingual.py' \\\n",
    "  --task 3 \\\n",
    "  --domain lap \\\n",
    "  --language eng \\\n",
    "  --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks_filtered.jsonl \\\n",
    "  --infer_data ../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_laptop_test_task3.jsonl \\\n",
    "  --bert_model_type microsoft/deberta-v3-base \\\n",
    "  --mode train \\\n",
    "  --epoch_num 3 \\\n",
    "  --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Verify Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submission_files = {\n",
    "    'Restaurant': 'tasks/subtask_3/pred_eng_restaurant.jsonl',\n",
    "    'Laptop': 'tasks/subtask_3/pred_eng_laptop.jsonl',\n",
    "}\n",
    "\n",
    "for domain, path in submission_files.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f'✗ {domain}: {path} NOT FOUND')\n",
    "        continue\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    total_quads = sum(len(d['Quadruplet']) for d in data)\n",
    "    with_quads = sum(1 for d in data if d['Quadruplet'])\n",
    "\n",
    "    # Validate format\n",
    "    errors = []\n",
    "    categories_seen = set()\n",
    "    for d in data:\n",
    "        for q in d['Quadruplet']:\n",
    "            # Check required fields\n",
    "            for field in ['Aspect', 'Category', 'Opinion', 'VA']:\n",
    "                if field not in q:\n",
    "                    errors.append(f\"Missing {field} in {d['ID']}\")\n",
    "            if 'Category' in q:\n",
    "                categories_seen.add(q['Category'])\n",
    "            # Check VA format\n",
    "            if 'VA' in q:\n",
    "                try:\n",
    "                    v, a = map(float, q['VA'].split('#'))\n",
    "                    if not (1.0 <= v <= 9.0 and 1.0 <= a <= 9.0):\n",
    "                        errors.append(f\"VA out of range in {d['ID']}\")\n",
    "                except:\n",
    "                    errors.append(f\"VA parse error in {d['ID']}\")\n",
    "\n",
    "    print(f'\\n✓ {domain}: {path}')\n",
    "    print(f'  Samples: {len(data)}')\n",
    "    print(f'  Total quadruplets: {total_quads}')\n",
    "    print(f'  Samples with quadruplets: {with_quads}/{len(data)}')\n",
    "    print(f'  Avg quads/sample: {total_quads/len(data):.2f}')\n",
    "    print(f'  Categories: {sorted(categories_seen)}')\n",
    "    print(f'  Errors: {len(errors)}')\n",
    "    if errors:\n",
    "        for e in errors[:5]:\n",
    "            print(f'    - {e}')\n",
    "    print(f'  First prediction: {json.dumps(data[0], ensure_ascii=False)[:250]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Cross-check IDs Against Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = {\n",
    "    'Restaurant': '../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_restaurant_test_task3.jsonl',\n",
    "    'Laptop': '../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_laptop_test_task3.jsonl',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for domain, test_path in test_files.items():\n",
    "    pred_path = submission_files[domain]\n",
    "    if not os.path.exists(pred_path) or not os.path.exists(test_path):\n",
    "        print(f'✗ {domain}: files missing')\n",
    "        all_ok = False\n",
    "        continue\n",
    "\n",
    "    with open(test_path, 'r') as f:\n",
    "        test_ids = {json.loads(line)['ID'] for line in f}\n",
    "    with open(pred_path, 'r') as f:\n",
    "        pred_ids = {json.loads(line)['ID'] for line in f}\n",
    "\n",
    "    missing = test_ids - pred_ids\n",
    "    extra = pred_ids - test_ids\n",
    "\n",
    "    if not missing and not extra:\n",
    "        print(f'✓ {domain}: All {len(test_ids)} IDs match')\n",
    "    else:\n",
    "        all_ok = False\n",
    "        if missing:\n",
    "            print(f'✗ {domain}: {len(missing)} missing IDs: {list(missing)[:5]}')\n",
    "        if extra:\n",
    "            print(f'✗ {domain}: {len(extra)} extra IDs: {list(extra)[:5]}')\n",
    "\n",
    "print(f'\\n{\"✓ SUBMISSION READY\" if all_ok else \"✗ FIX ISSUES ABOVE\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Copy Submission Files to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "out_dir = '/kaggle/working/submission_subtask3'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for domain, path in submission_files.items():\n",
    "    if os.path.exists(path):\n",
    "        dest = os.path.join(out_dir, os.path.basename(path))\n",
    "        shutil.copy2(path, dest)\n",
    "        print(f'✓ Copied {path} → {dest}')\n",
    "\n",
    "# Also save models\n",
    "model_dir = '/kaggle/working/trained_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "for f in ['model/task3_res_eng.pth', 'model/task3_lap_eng.pth']:\n",
    "    if os.path.exists(f):\n",
    "        shutil.copy2(f, os.path.join(model_dir, os.path.basename(f)))\n",
    "        print(f'✓ Copied {f}')\n",
    "\n",
    "print(f'\\nDownload from Output panel →')\n",
    "!ls -lh /kaggle/working/submission_subtask3/\n",
    "!ls -lh /kaggle/working/trained_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (Optional) Inference-Only Mode\n",
    "Use these cells if models are already trained and uploaded as a Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and set your model dataset path\n",
    "# MODEL_DATASET = '/kaggle/input/your-model-dataset'\n",
    "# \n",
    "# import shutil\n",
    "# os.makedirs('model', exist_ok=True)\n",
    "# for f in ['task3_res_eng.pth', 'task3_lap_eng.pth']:\n",
    "#     src = os.path.join(MODEL_DATASET, f)\n",
    "#     if os.path.exists(src):\n",
    "#         shutil.copy2(src, f'model/{f}')\n",
    "#         print(f'✓ Loaded {f}')\n",
    "# \n",
    "# # Inference only - Restaurant\n",
    "# !python 'run_task2&3_trainer_multilingual.py' \\\n",
    "#   --task 3 --domain res --language eng \\\n",
    "#   --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_restaurant_train_alltasks_filtered.jsonl \\\n",
    "#   --infer_data ../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_restaurant_test_task3.jsonl \\\n",
    "#   --bert_model_type microsoft/deberta-v3-base \\\n",
    "#   --mode inference\n",
    "# \n",
    "# # Inference only - Laptop\n",
    "# !python 'run_task2&3_trainer_multilingual.py' \\\n",
    "#   --task 3 --domain lap --language eng \\\n",
    "#   --train_data ../DimABSA2026/task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks_filtered.jsonl \\\n",
    "#   --infer_data ../DimABSA2026/task-dataset/track_a/subtask_3/eng/eng_laptop_test_task3.jsonl \\\n",
    "#   --bert_model_type microsoft/deberta-v3-base \\\n",
    "#   --mode inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
